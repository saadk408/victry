# Prompt for Optimizing CLAUDE.md Instructions

You are an expert in prompt engineering specializing in AI code assistant instructions. Your task is to deeply analyze and optimize the instructions for Claude Code in the @CLAUDE.md file.

Take a deep breath and take it one step at a time.

## Cognitive Framework
Before beginning, adopt a systematic optimization mindset:
- Think of instructions as a living system where each element influences the whole
- Question every assumption about what makes instructions effective
- Consider both immediate utility and long-term maintainability
- Recognize that the best instructions emerge from deep understanding of actual usage patterns
- Maintain awareness of your own biases and optimization blind spots
- Remember that instructions fail not from what they say, but from what they leave ambiguous
- Commit to grounding all decisions in the research you will conduct in Phase 1

## Context
Claude Code uses the CLAUDE.md file as its primary instruction set when working with a codebase. Effective instructions create a mental model that enables Claude Code to:
- **Navigate** the codebase as an experienced developer would
- **Anticipate** common patterns and potential issues
- **Adapt** its responses to the specific context of each task
- **Maintain** consistency with established practices
- **Evolve** its understanding as it encounters edge cases
- **Fail gracefully** when encountering ambiguous or novel situations

Consider also the human experience:
- Developers who will maintain and update these instructions
- The cognitive load of understanding and modifying the instruction set
- The balance between completeness and maintainability

## Your Task

### Phase 1: Research and Framework Development
Before analyzing the specific codebase, conduct thorough research using both your built-in web search and exa mcp to develop your analytical framework:

**Deep Research on CLAUDE.md Best Practices**:
- Research extensively online to understand what makes AI code assistant instructions effective
- Investigate how successful CLAUDE.md files are structured and why they work
- Study examples, case studies, and expert guidance on AI instruction design
- Explore the relationship between instruction patterns and AI performance
- Understand common pitfalls and anti-patterns in AI code assistant instructions
- Research how instructions interact as a system to create compound effects
- Gather insights on balancing comprehensiveness with usability
- Seek out both theoretical principles and practical implementation wisdom

**Important**: This research phase is critical—invest significant effort here as it forms the foundation for all subsequent work. Look for:
- Authoritative sources and recognized experts in AI instruction design
- Real-world examples with documented success metrics
- Comparative analyses of different instruction approaches
- Evidence-based principles, not just opinions

**Framework Synthesis**:
- Synthesize your research findings into core principles of instruction excellence
- Identify patterns that distinguish exceptional instruction sets from mediocre ones
- Map the relationship between instruction design choices and AI performance outcomes
- Develop specific criteria for evaluating instruction effectiveness
- Create a coherent theory of excellence based on your research findings

**Gap Identification and Additional Research**: 
- After initial research, identify any remaining gaps in your understanding
- Conduct targeted research to fill these gaps—never make assumptions
- Validate uncertain principles through additional sources
- Continue researching until you have a complete, evidence-based framework
- Document where specific insights come from to ensure grounded decisions

**Critical**: This research and synthesis becomes your primary analytical lens. Every evaluation, decision, and optimization in subsequent phases must be grounded in these research-based insights. Do not proceed until you have a comprehensive framework built from thorough research.

**Framework Validation**: Before moving to Phase 2, explicitly articulate:
- The key principles that define instruction excellence (based on your research)
- The specific criteria you'll use to evaluate effectiveness
- The patterns you'll look for that indicate success or failure
- How you'll recognize when instructions align with or violate best practices
- The sources and rationale behind your framework's core elements

### Phase 2: Deep Codebase Understanding
Immerse yourself in the codebase in the current working directory:

**Efficient Analysis Using Subagents**:
- Deploy subagents strategically to analyze different aspects of the codebase in parallel
- Have subagents focus on specific areas: architecture patterns, coding conventions, common workflows, technical debt, etc.
- Synthesize findings from multiple subagent analyses into a comprehensive understanding
- Use subagents to deep-dive into complex or critical parts of the system
- Ensure thorough coverage without redundant analysis

**Core Analysis Areas**:
- Map the conceptual architecture beyond just file structure
- Identify implicit patterns and conventions that aren't documented
- Discover the "personality" of the codebase - its quirks and philosophies
- Understand the evolution of the code and technical decisions made
- Recognize common developer workflows and pain points
- Note areas where Claude Code would most frequently operate
- Identify interaction patterns between different parts of the system
- Discover which aspects of the codebase are most fragile or complex
- Understand the boundary between what should be explicit vs. implicit knowledge

**Apply Phase 1 Framework**: As you explore, continuously reference your research-based framework. How do the codebase characteristics align with or challenge your framework? What unique aspects require special consideration?

### Phase 3: Current Instructions Evaluation Through Research Lens
Analyze the @CLAUDE.md file using your Phase 1 research framework as the primary evaluation criteria:
- **Intent vs. Best Practice**: Does each instruction align with research-based effectiveness principles?
- **Gap Analysis**: Using your framework, what critical elements are missing?
- **Anti-Pattern Detection**: Which sections violate known best practices from your research?
- **Value Density Assessment**: Where can instructions deliver more impact with fewer words?
- **Failure Mode Analysis**: How might instructions be misinterpreted? Where could literal interpretation cause problems?
- **Interaction Effects**: How do instructions work together? Where do they create confusion when combined?
- **Framework Alignment**: Rate each section against your synthesized criteria for excellence
- **Hidden Assumptions**: What knowledge is assumed but not stated?
- **Cognitive Load**: Which sections create unnecessary mental overhead?
- **Research-Based Improvements**: For each weakness, what does your research suggest as the solution?

**Validation Check**: Are you truly applying your Phase 1 insights, or falling back on generic assessment? Each evaluation should trace back to specific research-based principles.

### Phase 4: Research-Driven Optimization
Transform the instructions by applying your Phase 1 research findings:

**Foundation Setting**
- Apply your researched principles to establish what excellence looks like for this specific codebase
- Let your research guide decisions about scope, depth, and approach
- Use your framework to determine what deserves emphasis vs. what can be minimized

**Iterative Refinement**
- Each iteration should be guided by specific insights from your Phase 1 research
- Test proposed changes against your framework's criteria for effectiveness
- Ensure optimizations address root causes identified through research, not just symptoms
- Validate that changes align with proven patterns of success

**Integration Verification**
- Confirm that optimizations work as a coherent system, not just individual improvements
- Verify that the transformed instructions embody your research-based understanding
- Test whether the instructions enable the behaviors your framework predicts as successful

**Reflection Loop**
- Have you truly applied your research findings, or defaulted to generic improvements?
- Do the optimizations reflect deep understanding of what makes instructions effective?
- Would someone who shares your research knowledge recognize these as exemplary instructions?

## Optimization Principles
Let your Phase 1 research findings guide these principles in action:
- **Research-driven decisions**: Every choice should trace back to your researched best practices
- **Effectiveness through understanding**: Apply proven patterns from your framework
- **Value density from insight**: Use research knowledge to identify what truly matters
- **Clarity through tested principles**: Apply communication patterns known to work
- **Behavioral validation**: Verify changes against research-predicted outcomes
- **Contextual application**: Adapt universal principles to codebase-specific needs
- **System thinking**: Consider how your research shows instructions interact
- **Evolution readiness**: Build in flexibility your research identifies as crucial
- **Compound optimization**: Seek improvements that leverage multiple research insights

## Deliverables
Provide:
1. **Analysis Synthesis**: 
   - How your Phase 1 research framework revealed specific insights about the current instructions
   - Critical gaps or misalignments discovered through research-based analysis
   - Patterns identified by applying your researched best practices

2. **Optimized CLAUDE.md**: 
   - A transformed instruction set that embodies your research-based understanding
   - Instructions that demonstrate applied best practices throughout
   - Content that reflects deep integration of researched principles with codebase specifics

3. **Transformation Rationale**: 
   - How each major change stems from specific research findings
   - The framework principles that guided key decisions
   - Evidence of how optimizations embody researched best practices
   - Guidance for future maintainers on the research-informed approach taken

## Success Indicators
Your optimized instructions will be successful if Claude Code can:

**Behavioral Outcomes**
- Intuit the right approach for tasks without explicit guidance for every scenario
- Maintain the codebase's architectural integrity and coding philosophy
- Generate code that feels native to the project, not generic
- Handle edge cases gracefully by understanding underlying principles
- Ask clarifying questions that demonstrate deep understanding of the codebase

**Efficiency Metrics**
- Reduce back-and-forth clarifications by anticipating developer needs
- Minimize time spent searching for the right approach
- Enable rapid onboarding for new development patterns
- Support complex tasks without requiring instruction lookups

**Quality Indicators**
- Produce consistent outputs across similar tasks
- Avoid common pitfalls specific to the Victry codebase
- Adapt to new features or patterns by applying the established mental model
- Maintain code quality standards without explicit reminders

**Human Experience**
- Developers find the instructions intuitive to maintain and extend
- New team members can understand the codebase philosophy through the instructions
- Updates to instructions can be made surgically without cascading changes
- The instruction set serves as useful documentation beyond just AI assistance

## Final Consideration
Remember that the optimization process itself is iterative. The best instructions often emerge from understanding what doesn't work as much as what does. Trust your researched best practices while remaining grounded in the specific needs of the Victry codebase. The goal is not perfection, but a living document that significantly improves Claude Code's ability to be a valuable development partner.

**Critical Reminder**: Your Phase 1 research is not merely context—it is the intelligence that should inform every decision. If you find yourself making choices without clear connection to your research framework, pause and reconnect with those foundational insights. The quality of the final product directly correlates with how deeply you apply your research findings throughout the entire process.

## Meta-Considerations
Throughout this process, maintain continuous connection to your Phase 1 research:

**Research Application Checkpoint**
- Are you actively using insights from your Phase 1 research, or operating on assumptions?
- Do your decisions reflect deep understanding from your research findings?
- Would an expert in AI instruction design recognize your approach as research-informed?
- Are you discovering new insights that challenge or extend your research-based framework?

**Framework Evolution**
- How does this specific case study inform your understanding of best practices?
- What unique aspects of the Victry codebase reveal gaps in general principles?
- Where do theoretical best practices meet practical implementation challenges?
- How might this experience refine your framework for future use?

**Decision Integrity**
- Can you trace each optimization decision back to research-based principles?
- Are trade-offs being resolved using framework criteria or personal preference?
- Do your changes reflect synthesis of multiple research insights?
- Is the final product a true embodiment of excellence as defined by your research?

**Knowledge Transfer**
- Will the optimized instructions implicitly teach best practices to those who use them?
- Does the structure itself demonstrate research-based principles in action?
- Can future maintainers infer the underlying framework from the instructions?
- Do the instructions serve as a case study in applied best practices?

Remember: Your Phase 1 research isn't just preparation—it's the core intelligence that should permeate every aspect of this optimization.